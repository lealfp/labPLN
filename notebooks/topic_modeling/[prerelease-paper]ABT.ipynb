{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480b1d9f-3419-4a71-9348-913fdfa541a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "No sentence-transformers model found with name bert-base-cased. Creating a new one with mean pooling.\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "vocabulary = []    \n",
    "topics = []\n",
    "coherence = 0.0\n",
    "\n",
    "class Topic:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        # self.sentences = []\n",
    "        self.sentences = \"\"\n",
    "        self.words = []\n",
    "        self.coherence = 0.0\n",
    "\n",
    "\n",
    "\n",
    "#STEP 1 \n",
    "DATASET = 'clicr'\n",
    "DATASET_PATH = '../datasets/'+DATASET+'/cases-titles.txt'\n",
    "with open(DATASET_PATH) as f:\n",
    "\tcorpus = f.readlines()\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "BERT_MODEL = \"bert-base-cased\"\n",
    "# BERT_MODEL = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "language_model = SentenceTransformer(BERT_MODEL)\n",
    "vector_space = language_model.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfee7557-bd3a-48d8-b4fa-3be0a893ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nro de words para k=1: 12126\n",
      "top words:  50\n",
      "Words considered by ctfidf:  12126\n",
      "Total words:  12126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "['patient', 'syndrome', 'to', 'rare', 'case', 'as', 'unusual', 'acute', 'cause', 'disease', 'presenting', 'for', 'presentation', 'by', 'artery', 'after', 'following', 'induced', 'pulmonary', 'associated', 'treatment', 'cell', 'primary', 'carcinoma', 'tumour', 'diagnosis', 'old', 'bilateral', 'management', 'due', 'secondary', 'year', 'spontaneous', 'from', 'report', 'infection', 'renal', 'cancer', 'pain', 'severe', 'young', 'non', 'child', 'cyst', 'recurrent', 'coronary', 'abdominal', 'pregnancy', 'chronic', 'mimicking']\n",
      "Total coherence:  0.1355571139830774\n",
      "Coherence by topic:  [0.1355571139830774]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "vocabulary = []    \n",
    "topics = []\n",
    "coherence = 0.0\n",
    "\n",
    "class Topic:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "        self.sentences = \"\"\n",
    "        self.words = []\n",
    "        self.coherence = 0.0\n",
    "\n",
    "\n",
    "\n",
    "### STEP 1 \n",
    "DATASET = 'clicr'\n",
    "DATASET_PATH = '../datasets/'+DATASET+'/cases-titles.txt'\n",
    "with open(DATASET_PATH) as f:\n",
    "\tcorpus = f.readlines()\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "BERT_MODEL = \"bert-base-cased\"\n",
    "language_model = SentenceTransformer(BERT_MODEL)\n",
    "vector_space = language_model.encode(corpus)\n",
    "\n",
    "# In order to use locally stored Language Models\n",
    "# MODEL_PATH = '../../models/' + BERT_MODEL\n",
    "# language_model = SentenceTransformer(MODEL_PATH)\n",
    "# vector_space = language_model.encode(model.sentences)\n",
    "\n",
    "### STEP 2\n",
    "DISTANCE_THRESHOLD =300\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering_model = AgglomerativeClustering(linkage=\"ward\", distance_threshold=DISTANCE_THRESHOLD, n_clusters=None)\n",
    "clustering_model = clustering_model.fit(vector_space)\n",
    "k = clustering_model.n_clusters_\n",
    "\n",
    "topics = [Topic(i) for i in range(k)]\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    inferred_cluster_index = clustering_model.labels_[i]\n",
    "    topics[inferred_cluster_index].sentences += corpus[i] + \" \"\n",
    "\n",
    "\n",
    "\n",
    "### STEP 3\n",
    "MAX_DF = 0.1\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "if (k > 1):\n",
    "    c_MAX_DF = 0.5\n",
    "    c_tfidf_model = TfidfVectorizer(max_df=c_MAX_DF, smooth_idf=True, use_idf=True)\n",
    "    c_tfidf = c_tfidf_model.fit_transform([topic.sentences for topic in topics])\n",
    "    c_tfidf_matrix = c_tfidf.toarray()\n",
    "    words = c_tfidf_model.get_feature_names_out()\n",
    "                    \n",
    "    for i, topic in enumerate(topics):\n",
    "        sorted_term_indexes = np.argsort(-1*c_tfidf_matrix[topic.index])\n",
    "        topic.words = [words[j] for j in sorted_term_indexes]\n",
    "else:\n",
    "    c_tfidf_model = TfidfVectorizer(max_df=MAX_DF,smooth_idf=True, use_idf=True)\n",
    "    c_tfidf = c_tfidf_model.fit_transform(corpus)\n",
    "    c_tfidf_matrix = c_tfidf.toarray()\n",
    "    words = c_tfidf_model.get_feature_names_out()\n",
    "    \n",
    "    mean_tfidf = np.array(c_tfidf_matrix.mean(axis=0)).flatten()\n",
    "    sorted_term_indexes = np.argsort(-1*mean_tfidf)\n",
    "    topics[0].words = [words[j] for j in sorted_term_indexes]\n",
    "\n",
    "\n",
    "# VALIDATION\n",
    "TOP_WORDS = 50\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "tfidf_model = TfidfVectorizer(max_df=MAX_DF, smooth_idf=True, use_idf=True)\n",
    "tfidf = tfidf_model.fit_transform(corpus)\n",
    "vocabulary = tfidf_model.get_feature_names_out()\n",
    "terms_by_sentence = tfidf_model.inverse_transform(tfidf)\n",
    "        \n",
    "dictionary = Dictionary(terms_by_sentence)\n",
    "cm = CoherenceModel(topics=[topic.words[:TOP_WORDS] for topic in topics], texts=terms_by_sentence, dictionary=dictionary, coherence='c_v', topn=TOP_WORDS)\n",
    "coherence = cm.get_coherence()\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "\n",
    "for i, coherence in enumerate(coherence_per_topic):\n",
    "    topics[i].coherence = coherence\n",
    "    print(\"Topic \", str(i))\n",
    "    print(topics[i].words[:TOP_WORDS])\n",
    "\n",
    "print(\"Total coherence: \", cm.get_coherence())\n",
    "print(\"Coherence by topic: \", cm.get_coherence_per_topic()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16277c-9470-4370-876e-3db64bd05d8f",
   "metadata": {},
   "source": [
    "## Ordenar os tópicos por coerência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5400f1-7542-4f75-8e90-93235e636a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# print(model.topics[0].coherence)\n",
    "# print(model.topics[1].coherence)\n",
    "\n",
    "topics = sorted(topics, key=lambda topic: topic.coherence, reverse=True)\n",
    "# print(model.topics)\n",
    "print(topics[1].coherence)\n",
    "print(topics[1].sorted_terms[:10])\n",
    "print(topics[1].sorted_tfidf[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64720d31-b99d-417b-9039-493efc351fc1",
   "metadata": {},
   "source": [
    "## Encontrar sentenças contendo determinadas palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61fec8-ebda-4d57-956e-43e6ddd4c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in model.topics:\n",
    "    print('Topic ',t.cluster)\n",
    "\n",
    "    print(t.sorted_terms[:10])\n",
    "    print(t.sorted_tfidf[:10])\n",
    "    \n",
    "    print(t.sorted_terms[-1])\n",
    "    print(t.sorted_tfidf[-1])\n",
    "\n",
    "    print(len(t.sentences))\n",
    "    \n",
    "    # if any(\"osteogenesis\" in s for s in t.sentences):\n",
    "    #     print(\"aquiiiiiii\")\n",
    "    #     print(s)\n",
    "\n",
    "    matches = [match for match in t.sentences if \"arteriovenous\" in match]\n",
    "     \n",
    "    print(matches)\n",
    "    \n",
    "    print(t.sorted_terms[-2])\n",
    "    print(t.sorted_tfidf[-2])\n",
    "    # print(len(t.sorted_terms))\n",
    "    print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec93fd63-f972-4212-8ed5-c67072f3d7ba",
   "metadata": {},
   "source": [
    "---------------\n",
    "# comparações feitas entre o ABT, LDA e Randômico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6aca62-255a-4a6e-9ec7-72244cf11fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.4034653288931263\n",
    "\n",
    "\n",
    "k = 1, 2,   200\n",
    "TW 10,   tfidf=0.015   ctfidf=0.9\n",
    "0.32, 0.51,     ,   0.36             \n",
    "\n",
    "TW 10,   tfidf=0.01   ctfidf=0.5\n",
    "meu = 0.32  ,  0.51,   0.35   \n",
    "LDA = 0.38, 0.20, 0.39\n",
    "\n",
    "TW 10, tfidf=0.95    ctfidf=0.5\n",
    "meu 0.35, 0.51, 0.36\n",
    "lda= 0.36, 0.35 ,0.37\n",
    "random = 0.51, 0.52, \n",
    "\n",
    "TW 100, tfidf=0.015    ctfidf=0.5\n",
    "meu 0.59  0.90 , 0.81\n",
    "lda = 0.27, 0.36 0.80\n",
    "random = 0.98, 0.93, \n",
    "\n",
    "# ----------\n",
    "# em belem\n",
    "TW 20, tfidf=0.015    ctfidf=0.99\n",
    "meu  0.22, 0.65 , 0.41\n",
    "lda = 0.31, 0.34,    , 0.62\n",
    "random = 0.72  0.72      \n",
    "\n",
    "TW 20, tfidf=0.015    ctfidf=0.5\n",
    "meu 0.22, 0.64, 7=0.7096830441426468, 0.41\n",
    "lda = 0.31, 0.34,    , 0.62\n",
    "random = 0.72  0.72     \n",
    "\n",
    "TW 20, tfidf=0.015    ctfidf=0.99\n",
    "meu 0.22, 0.64, 7= 0.40, 0.48771604676442953\n",
    "lda = 0.31, 0.34,    , 0.62\n",
    "random = 0.72  0.72    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dff428-b464-4af1-8e62-4fbc7795b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# TOP_WORDS = 10\n",
    "k = 2\n",
    "\n",
    "random_topics = []\n",
    "for i in range(k):\n",
    "    r = vocabulary.copy()\n",
    "    # r = topics[i].words.copy()\n",
    "    \n",
    "    random.shuffle(r)\n",
    "    random_topics.append(r[:TOP_WORDS])\n",
    "\n",
    "print(random_topics)\n",
    "\n",
    "cm = CoherenceModel(topics=random_topics, texts=terms_by_sentence, dictionary=dictionary, coherence=\"c_v\",topn=TOP_WORDS)\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "\n",
    "print(coherence)\n",
    "print(coherence_per_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd97529-ca2b-4f49-9b65-06a64ed4c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test_topics = []\n",
    "\n",
    "r1 = ['syndrome', 'rare', 'presenting', 'patient', 'carcinoma', 'case', 'cell', 'bilateral', 'spontaneous', 'unusual', 'cause', 'diagnosis', 'renal', 'primary', 'presentation', 'acute', 'management', 'pregnancy', 'cancer', 'following']\n",
    "r2 = ['after', 'patient', 'following', 'vein', 'induced', 'injury', 'rare', 'thrombosis', 'treatment', 'pulmonary', 'secondary', 'primary', 'fracture', 'cause', 'syndrome', 'tumour', 'lymphoma', 'associated', 'haemorrhage', 'adult']\n",
    "r3 = ['case', 'patient', 'report', 'unusual', 'review', 'is', 'literature', 'rare', 'syndrome', 'disease', 'cause', 'acute', 'not', 'two', 'induced', 'bilateral', 'right', 'coronary', 'it', 'pain']\n",
    "r4 = ['old', 'year', 'artery', 'acute', 'patient', 'unusual', 'presentation', 'rare', 'presenting', 'syndrome', 'woman', 'cerebral', 'after', 'cell', 'disease', 'non', 'severe', 'cause', 'aneurysm', 'multiple']\n",
    "r5 = ['patient', 'acute', 'treatment', 'following', 'disease', 'rare', 'infection', 'management', 'syndrome', 'associated', 'presenting', 'endocarditis', 'valve', 'after', 'obstruction', 'pulmonary', 'injury', 'intestinal', 'necrotising', 'severe']\n",
    "\n",
    "test_topics.append(r1[:TOP_WORDS])\n",
    "test_topics.append(r2[:TOP_WORDS])\n",
    "test_topics.append(r3[:TOP_WORDS])\n",
    "test_topics.append(r4[:TOP_WORDS])\n",
    "test_topics.append(r5[:TOP_WORDS])\n",
    "# test_topics.append(r6[:TOP_WORDS])\n",
    "# test_topics.append(r7[:TOP_WORDS])\n",
    "\n",
    "print(test_topics)\n",
    "\n",
    "dictionary = Dictionary(terms_by_sentence)\n",
    "cm = CoherenceModel(topics=test_topics, texts=terms_by_sentence, dictionary=dictionary, coherence=\"c_v\",topn=TOP_WORDS)\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "\n",
    "print(coherence)\n",
    "print(coherence_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a15cc-e860-48b1-9851-0dd6cc96697f",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb78b5-75cb-44b1-b551-edddda7f12ca",
   "metadata": {},
   "source": [
    "### Álgebra sobre espaços de sentenças e de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9e0ab-d52a-4ffc-b2c0-95c1e59b3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.inner(embeddings[0],embeddings[1])\n",
    "\n",
    "# MODEL = 'biobert-base'\n",
    "# MODEL = 'bert-base-cased'\n",
    "WORD_MODEL = 'ACD'\n",
    "\n",
    "WORD_EMBEDDINGS_TSV_INPUT_PATH = '../from_embeddings_to_tsv/output_from_1/' + WORD_MODEL + '/'\n",
    "\n",
    "word_embeddings_dataframe = pd.read_csv(WORD_EMBEDDINGS_TSV_INPUT_PATH + 'word_embeddings.tsv', sep='\\t',header=None)\n",
    "word_embeddings_numpy = word_embeddings_dataframe.to_numpy()\n",
    "word_embeddings = word_embeddings_numpy[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b27b19-65c6-4bea-a003-0ca5677fb3d0",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0549e8-f7f8-4c81-8516-35ee6084fc0c",
   "metadata": {},
   "source": [
    "#### Buscando as palavras (i.e., BERT terms) mais próximas de uma certa sentença"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0f298-a6c4-4a9b-a410-7ccad7ab179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDINGS_TSV_INPUT_PATH = '../from_embeddings_to_tsv/output_from_1/' + WORD_MODEL + '/labels.tsv'\n",
    "\n",
    "terms = []\n",
    "with open(WORD_EMBEDDINGS_TSV_INPUT_PATH) as f:\n",
    "    terms = f.readlines()\n",
    "\n",
    "val = np.inner(embeddings[0],word_embeddings[1])\n",
    "val\n",
    "\n",
    "from numpy import argsort\n",
    "\n",
    "dists = [''] * len(word_embeddings)\n",
    "dist_indexes = [''] * len(dists)\n",
    "\n",
    "for i, embedding in enumerate(word_embeddings):\n",
    "    dist = np.inner(embeddings[1],embedding)\n",
    "    dists[i] = dist\n",
    "    \n",
    "sorted_dists = sorted(dists)\n",
    "args = argsort(dists)\n",
    "        \n",
    "with open(OUTPUT_PATH + '/mainterms.txt', \"a\") as file:\n",
    "    print('Word: '+WORD_MODEL, file=file)  \n",
    "    print('Sentence: ' + sentences[1], file=file)\n",
    "    for a in args:\n",
    "        if not terms[a].startswith('##'):\n",
    "            print(terms[a], file=file)   \n",
    "            i = i+1\n",
    "            if i>100:\n",
    "                break;\n",
    "\n",
    "    print('----------------------------------------------------------------------------', file=file)  \n",
    "\n",
    "sorted_dists[:10]\n",
    "terms[17730]\n",
    "\n",
    "sorted_topic_indexes = np.argsort(model.coherence_per_topic)\n",
    "for i,j in enumerate(sorted_topic_indexes):    \n",
    "    print(i, model.topics[j].coherence, model.topics[j].sorted_terms[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce40bb-033d-4d7a-b48f-32a74e50e1d4",
   "metadata": {},
   "source": [
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b21a3-16f3-44d6-9e4d-283e51b52a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### corrigir ordem dos termos. Ja esta corrigido no notebook oficial e no artigo\n",
    "print('Topics in ascending sort of coherence.')\n",
    "sorted_topic_indexes = np.argsort(coherence_per_topic)\n",
    "for i,j in enumerate(sorted_topic_indexes):    \n",
    "    print(i, topics[j].coherence, topics[j].sorted_terms[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
