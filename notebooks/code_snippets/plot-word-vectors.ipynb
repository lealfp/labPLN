{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e020df-8a62-428b-aa63-d172ddcf1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# MODEL_PATH = 'fagner/envoy'\n",
    "MODEL_PATH = 'bert-base-cased'\n",
    "# MODEL_PATH = 'openai-community/gpt'\n",
    "# MODEL_PATH = 'HeroGeonil/Hypert-medical'\n",
    "\n",
    "# MODEL_PATH = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False)\n",
    "model = BertModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "embeds = model.state_dict()['embeddings.word_embeddings.weight'] \n",
    "\n",
    "# words = list(tokenizer.vocab.keys())\n",
    "\n",
    "X_reduzido = embeds[:10000]\n",
    "# words_reduzido = words[:10000]\n",
    "# X_reduzido = embeds\n",
    "# words_reduzido = words\n",
    "\n",
    "# print('Tamanho: ',len(words_reduzido))\n",
    "\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())  \n",
    "\n",
    "from sklearn import manifold\n",
    "\n",
    "X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X_reduzido)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "IMG_PATH = 'imgs/'\n",
    "\n",
    "if not os.path.exists(IMG_PATH):\n",
    "    os.mkdir(IMG_PATH)\n",
    "\n",
    "IMG_NAME = MODEL_PATH.replace('/', \"-\").replace('.', \"-\")\n",
    "print(IMG_NAME)\n",
    "\n",
    "# def plot_clustering(X_red, labels, title=None):\n",
    "# plt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n",
    "\n",
    "def plot_clustering(X_red, title=None):\n",
    "    # X_grandao = X_red\n",
    "    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n",
    "    X_red = (X_red - x_min) / (x_max - x_min)\n",
    "    # X_grandao.append(X_red)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # for i in range(3)\n",
    "    # X_reduzido = embeds[:10000]\n",
    "    # words_reduzido = words[:10000]\n",
    "\n",
    "    # X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X_reduzido)\n",
    "    \n",
    "    # print(X_red)\n",
    "    # plt.scatter(X_red[:, 0], X_red[:, 1], color='black', s=3)\n",
    "    # np.concatenate(X_grandao , X_red)\n",
    "    # print(X_red)\n",
    "    # X_reduzido = embeds[10001:20000]\n",
    "    # words_reduzido = words[10001:20000]\n",
    "\n",
    "    # X_red1 = manifold.SpectralEmbedding(n_components=2).fit_transform(X_reduzido)\n",
    "    # x_min, x_max = np.min(X_red1, axis=0), np.max(X_red1, axis=0)\n",
    "    # X_red1 = (X_red1 - x_min) / (x_max - x_min)\n",
    "    # # X_red.append(X_red1)\n",
    "    # X_red = np.append(X_red , X_red1, axis=0)\n",
    "    # print(X_red)\n",
    "    # # numpy.append(M, a)\n",
    "    # # plt.scatter(X_red[:, 0], X_red[:, 1], color='black', s=3)\n",
    "\n",
    "    # X_reduzido = embeds[20001:len(embeds)]\n",
    "    # words_reduzido = words[20001:len(embeds)]\n",
    "\n",
    "    # X_red2 = manifold.SpectralEmbedding(n_components=2).fit_transform(X_reduzido)\n",
    "    # x_min, x_max = np.min(X_red2, axis=0), np.max(X_red2, axis=0)\n",
    "    # X_red2 = (X_red2 - x_min) / (x_max - x_min)\n",
    "    # X_red = np.append(X_red , X_red2, axis=0)\n",
    "\n",
    "    \n",
    "    # print(X_red)\n",
    "    \n",
    "    plt.scatter(X_red[:, 0], X_red[:, 1], color='black', s=3)\n",
    "    \n",
    "    # plt.scatter(X_red[:, 0], X_red[:, 1], color='black', s=3)\n",
    "\n",
    "    \n",
    "#     for i in range(X_red.shape[0]):\n",
    "# #         print(X_red[i, 0])\n",
    "# #         print(X_red[i, 1])\n",
    "#         plt.text(X_red[i, 0], X_red[i, 1], str(words_reduzido[i]),\n",
    "#                  # color=plt.cm.nipy_spectral(labels[i] / 10.),\n",
    "#                  color=plt.cm.nipy_spectral(0 / 10.),\n",
    "#                  fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title, size=17)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(IMG_PATH + title)\n",
    "\n",
    "plot_clustering(X_red, IMG_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4fdb707-41cb-4a43-9155-ee298443a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at HeroGeonil/Hypert-medical and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'HeroGeonil/Hypert-medical'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'HeroGeonil/Hypert-medical' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, models\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained('HeroGeonil/Hypert-medical')\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m word_embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHeroGeonil/Hypert-medical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mPooling(word_embedding_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension())\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(modules\u001b[38;5;241m=\u001b[39m[word_embedding_model, pooling_model])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:38\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer_name_or_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# No max_seq_length set. Try to infer from model\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:786\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2008\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2002\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2003\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load following files from cache: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munresolved_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and cannot check if these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2005\u001b[0m     )\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 2008\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2009\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2011\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2012\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2013\u001b[0m     )\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'HeroGeonil/Hypert-medical'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'HeroGeonil/Hypert-medical' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "# tokenizer = AutoTokenizer.from_pretrained('HeroGeonil/Hypert-medical')\n",
    "\n",
    "word_embedding_model = models.Transformer('HeroGeonil/Hypert-medical', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4676f28-c445-4f47-8d69-ea2a0c3297bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_reduzido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e556b-20b4-4df6-8e67-e1ebc465dbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882babf-5383-4f7b-8e15-73af3590cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0d035-305b-4b90-9133-d8c2a619bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Glyph 146 missing from current font.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368b692-71e0-439a-97b4-0207df75dad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7d446-f855-4b6c-830b-1477d1d599e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3410306-2373-4617-b634-4df7fe788845",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install font-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b982ee6-0bf9-4dc8-b619-a0014cc7985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install font-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a19ae2-75e4-43dd-8999-f7d70c572dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -S apt-get install font-manager < mypasswordfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e2b75-ef9c-4229-9026-2e3ff275ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "command = \"sudo -S apt install msttcorefonts -qq\" # can be any command but don't forget -S as it enables input from stdin\n",
    "os.popen(command, 'w').write(password+'\\n') # newline char is important otherwise prompt will wait for you to manually perform newline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d745c01-8729-4796-b2b5-0a516799404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a859c5-af83-46d0-86c2-22b091adc644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
