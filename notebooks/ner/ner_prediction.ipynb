{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compact-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.24.4)\n",
      "Collecting torch\n",
      "  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m255.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:57\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.4.1-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m629.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "Successfully installed torch-2.4.1 torchaudio-2.4.1 torchvision-0.19.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silver-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = 'fine_tuned/NER/ACD_1epoch'\n",
    "# MODEL = 'fine_tuned/NER/ACD_5epochs'\n",
    "# MODEL = 'fine_tuned/NER/ACD_10epochs'\n",
    "\n",
    "# MODEL = 'fine_tuned/NER/ACD'\n",
    "# MODEL = 'ACD(overfited)'\n",
    "\n",
    "# MODEL_PATH = '../../models/' + MODEL\n",
    "\n",
    "MODEL_PATH = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "young-trustee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "labels = []\n",
    "labels = [value for k, value in config.id2label.items()]\n",
    "\n",
    "print(config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attractive-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False)\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(labels),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tamil-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa o dataframe por PONTO-FINAL\n",
    "def separar_frases(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "instructional-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    print(sentence)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence:\n",
    "#         print('sentence ', word)\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "#         print(word)\n",
    "#         break\n",
    "        tokenized_word = tokenizer.tokenize(str(word))\n",
    "        print(tokenized_word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        \n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "#         # Add the same label to the new list of labels `n_subwords` times\n",
    "#         if label.startswith(\"B\"):\n",
    "#             labels.extend([label])\n",
    "#             new_label = \"I-\" + label[2:]\n",
    "\n",
    "#             labels.extend([new_label] * (n_subwords-1))\n",
    "#         else:\n",
    "#             labels.extend([label] * n_subwords)\n",
    "\n",
    "\n",
    "    return tokenized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "objective-methodology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0]\n",
      "eurological\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "test_sentence = \"Neurological\"\n",
    "\n",
    "\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "\n",
    "# input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "input_ids = torch.tensor([tokenized_sentence])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "versum = \"\"\n",
    "\n",
    "print(label_indices[0])\n",
    "# print(len(tokens))\n",
    "\n",
    "# for token, label_idx in zip(tokens, label_indices[0]):\n",
    "entitying = False\n",
    "change_of_label = False\n",
    "current_label = \"\"\n",
    "for idx, (token, label_idx) in enumerate(zip(tokens, label_indices[0])):\n",
    "#     print(token)    \n",
    "#     print(label_idx)\n",
    "#     print(labels[label_idx])\n",
    "\n",
    "#     print()\n",
    "    if (token == \"[CLS]\" or token == \"[SEP]\"):\n",
    "        continue\n",
    "        \n",
    "    if token.startswith(\"##\"):\n",
    "        sub_token = True\n",
    "        versum = versum + token[2:]\n",
    "#         new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        if (labels[label_idx].startswith(\"B\")):\n",
    "            if entitying:\n",
    "                versum += '}(' + entity +')'\n",
    "                \n",
    "            entitying = True\n",
    "            versum = versum + ' {' + token\n",
    "#             print(versum)\n",
    "            entity = labels[label_idx][2:]\n",
    "            \n",
    "        \n",
    "        if labels[label_idx].startswith(\"O\") :\n",
    "            if entitying:\n",
    "#                 print(entity)\n",
    "                versum = versum + '}(' + entity +') ' + token\n",
    "            else:\n",
    "                versum = versum + ' ' + token\n",
    "            entitying = False\n",
    "\n",
    "                \n",
    "        if labels[label_idx].startswith(\"I\") :\n",
    "            if previous_label[2:] != labels[label_idx][2:]:\n",
    "                versum += '}(' + entity +') {'\n",
    "                entity = labels[label_idx][2:]\n",
    "                change_of_label = True\n",
    "#                 entitying = False\n",
    "            else: change_of_label = False\n",
    "                \n",
    "#             print(labels[label_idx][2:])\n",
    "#             if entitying:\n",
    "            if change_of_label:\n",
    "                versum += token\n",
    "                entitying = True\n",
    "            else:\n",
    "    #       print(entity)\n",
    "    #                 versum = versum + '}(' + entity +')'\n",
    "                print('aqui')\n",
    "                entitying = True\n",
    "                versum += ' ' + token\n",
    "\n",
    "#             else:\n",
    "#                 versum = versum + ' ' + token\n",
    "    previous_label = labels[label_idx]\n",
    "        \n",
    "#             new_labels.append(labels[label_idx])\n",
    "#             new_tokens.append(token)\n",
    "#     print(versum)\n",
    "\n",
    "    \n",
    "print(versum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a59b91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'N', '##eur', '##ological', '[SEP]']\n",
      "[CLS]\t\tLABEL_0\n",
      "Neurological\t\tLABEL_1\n",
      "[SEP]\t\tLABEL_0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# test_sentence = 'Doctor, I am feeling chest pain since yesterday. The pain is continuous and is located just in the middle of my chest, worsening when I breathe and when I lay down on my bed. I suffer from arterial hypertension and smoke 20 cigarettes every day. My father had a “heart attack” at my age and I am very worried about it.'\n",
    "# test_sentence = \" Her eye is green. A 58-year-old African-American woman presents to the ER with episodic pressing/burning anterior chest pain that began two days earlier for the first time in her life. The pain started while she was walking, radiates to the eyes, and is accompanied by nausea, diaphoresis and mild dyspnea, but is not increased on inspiration. The latest episode of pain ended half an hour prior to her arrival. She is known to have hypertension and obesity. She denies smoking, diabetes, hypercholesterolemia, or a family history of heart disease. She currently takes no medications. Physical examination is normal. The EKG shows nonspecific changes. Alprazolan was administred.\"\n",
    "# test_sentence = 'Recently, we found that therapy with selegiline and L-dopa was associated with selective systolic orthostatic hypotension which was abolished by withdrawal of selegiline. '\n",
    "# test_sentence = 'The cells were examined in a Zeiss LSM 510 laser scanning microscope equipped with a Plan - Apochromate 63x/1.4 oil immersion objective.'\n",
    "# test_sentence = 'Torsade de pointes ventricular tachycardia during  low dose intermittent dobutamine treatment in a patient with dilated cardiomyopathy and congestive heart failure.'\n",
    "# test_sentence = 'A DNA molecule'\n",
    "# test_sentence = '(a) Schematic drawing of the magnetic tweezers.'\n",
    "# test_sentence = 'Doctor, I am feeling a very strong chest pain in the middle of my chest. I have arterial hypertension but I don’t take my medication every day.'\n",
    "# test_sentence = 'Doctor, I am feeling a very strong chest pain in the middle of my chest. It has started 2 hours ago, but its intensity isn’t decreasing. It started suddenly and I am also feeling pain in my back and my neck. The pain is continuous, and it did not worse when I am breathing. I have arterial hypertension but I don’t take my medication every day, I am sorry! I don’t smoke, and I don’t know If I have other diseases. My father has also arterial hypertension.'\n",
    "# test_sentence = 'Doctor, I am feeling a very strong chest pain in the middle of my chest. It started suddenly and I am also feeling pain in my back and my neck. I have arterial hypertension but I don’t take my medication every day.'\n",
    "\n",
    "test_sentence = 'Neurological'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(test_sentence)\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "# print(tokenized_sentence)\n",
    "# input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "input_ids = torch.tensor([tokenized_sentence])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "# print(output)\n",
    "\n",
    "\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "# print(label_indices)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "print(tokens)\n",
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "#     print(label_idx)\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "#         print('else')\n",
    "#         print(new_labels)\n",
    "#         print(new_tokens)\n",
    "        new_labels.append(labels[label_idx])\n",
    "        new_tokens.append(token)\n",
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t\\t{}\".format(token, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cross-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = '[CLS] '\n",
    "# test_sentence += 'In two of the three deaths probably associated with ketoconazole treatment the drug had been continued after the onset of jaundice and other symptoms of hepatitis.'\n",
    "test_sentence += 'Two patients with type II diabetes mellitus developed and acute hepatites-like syndrome soon after initiation of glyburide therapy.'\n",
    "test_sentence += '[SEP]'\n",
    "words = separar_frases(test_sentence)\n",
    "# tokenize(words)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "micro-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word in sentence:\n",
    "#         print('sentence ', word)\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "#         print(word)\n",
    "#         break\n",
    "        tokenized_word = tokenizer.tokenize(str(word))\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "#         # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "#         # Add the same label to the new list of labels `n_subwords` times\n",
    "#         if label.startswith(\"B\"):\n",
    "#             labels.extend([label])\n",
    "#             new_label = \"I-\" + label[2:]\n",
    "\n",
    "#             labels.extend([new_label] * (n_subwords-1))\n",
    "#         else:\n",
    "#             labels.extend([label] * n_subwords)\n",
    "\n",
    "\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-reply",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-marriage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
